xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
?lapply
2623.11*250
2623.11*250/60/60
185/24
183/24
6*70
6*90
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetSeq", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
conditions
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
conditions
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
out
require(lavaan)
install.packages("lavaan")
require(lavaan)
load(HolzingerSwineford1939)
head(HolzingerSwineford1939)
unique(HolzingerSwineford1939[, 8])
length(unique(HolzingerSwineford1939[, 8]))
plot(HolzingerSwineford1939[, "x2"])
hist(HolzingerSwineford1939[, "x2"])
hist(HolzingerSwineford1939[, "x1"])
hist(HolzingerSwineford1939[, "x3"])
hist(HolzingerSwineford1939[, "x9"])
require(lavaan)
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit <- cfa(HS.model, data=HolzingerSwineford1939)
lavInspect(fit, "sampstat")
covmat <- lavInspect(fit, "sampstat")
covmat$cov
solve(covmat$cov)
covmat$cov[,1]
cfs <- solve(covmat$cov) %*% covmat$cov[,1]
cfs
cfs <- solve(covmat$cov) %*% covmat$cov[,2]
solve(covmat$cov) %*% covmat$cov[,2]
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
summary(fit)
coef(fit)
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
0.809*0.554
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
0.549*0.809*0.554*1.134
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 2
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
(.42 + .83 * 0)*(.04+.28*0)
(.42 + .83 * 1)*(.04+.28*1)
7.007/38.925
7.007/18
1.157/8
.883/8
.714^2 + .371^2
26.44/7
26/7
##### Install package -----
setwd("~/Documents/bayesreg")
library(bayesreg)
##### Simulate data -----
library(MASS) # for mvrnorm function in sim.lm
set.seed(16102018)
sim.lm <- function(nobs, cfs, sd, cov.mat){
nvar <- length(cfs)
beta <- as.matrix(cfs)
X <- mvrnorm(nobs, mu=rep(0, nvar), Sigma=cov.mat)
y <- X %*% beta + rnorm(nobs,0,sd)
return (list(x=X, y=y, coefficients=cfs, var=sd^2, covX=cov.mat))
}
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
colnames(dat$x) <- c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8")
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "lasso", missing = FALSE, chains = 2)
select_lm(fit, X = dat$x, prob = 0.9)
pmse_lm(fit, y = dat$y, N_train = 40)
varnms <- colnames(dat$x)
draws = rstan::extract(object, pars = "mu", "beta", "sigma")
draws = rstan::extract(fit, pars = "mu", "beta", "sigma")
draws = rstan::extract(fit, pars = c("mu", "beta", "sigma"))
apply(draws[[1]], 2, sd)
posterior_interval(draws[[1]])
require(rstantools)
posterior_interval(draws[[1]])
posterior_interval(draws$mu[[1]])
draws = rstan::extract(fit, pars = "mu")
posterior_interval(draws$mu[[1]])
posterior_interval(draws[[1]])
require(rstan)
draws = rstan::extract(fit, pars = c("mu", "beta", "sigma"))
posterior_interval(draws$mu)
posterior_interval(draws$mu[[1]])
posterior_interval(draws[[1]])
mu <- as.matrix(draws$mu)
posterior_interval(mu)
posterior_interval(mu, prob = c(0.1, 0.2))
print(fit)
summary(fit)
?summary
rstan::summary()
rstan::summary
showMethods("summary")
head(mu)
################################################################################################################
draws = rstan::extract(object, pars = "beta")
################################################################################################################
draws = rstan::extract(fit, pars = "beta")
posterior_interval(draws[[1]])
posterior_interval(draws[[1]], prob = 0.95)
median = posterior_interval(draws[[1]], prob=0.5)
median
median = posterior_interval(draws[[1]], prob = 0)
median = posterior_interval(draws[[1]], prob = 0.1)
median = apply(draws[[1]], 2, median)
median
sd = apply(draws[[1]], 2, sd)
################################################################################################################
draws = rstan::extract(fit, pars = "beta")
head(draws[[1]])
################################################################################################################
draws = rstan::extract(fit, pars = "mu")
median = apply(draws[[1]], 2, median)
head(draws[[1]])
mean(draws[[1]])
posterior_interval(draws[[1]], prob = 0.95)
posterior_interval(as.matrix(draws[[1]]), prob = 0.95)
print(fit, pars="mu")
################################################################################################################
draws = rstan::extract(fit, pars = "beta")
ncol(draws[[1]])
##### Install package -----
setwd("~/Documents/bayesreg")
devtools::document() # update documentation
devtools::install(local=TRUE) # use FALSE if stan models need to be recompiled
devtools::document() # update documentation
devtools::install(local=TRUE) # use FALSE if stan models need to be recompiled
library(bayesreg)
summary_lm(fit)
object <- fit
CI <- 0.95
# results intercept
intercept = rstan::extract(object, pars = "mu")
mean.int = mean(intercept[[1]])
median.int = median(intercept[[1]])
sd.int = sd(intercept[[1]])
ci.int = posterior_interval(as.matrix(intercept[[1]]), prob = CI)
res.int = c(mean.int, median.int, sd.int, ci.int)
res.int
# results regression coefficients
beta = rstan::extract(object, pars = "beta")
mean.beta = apply(beta[[1]], 2, mean)
median.beta = apply(beta[[1]], 2, median)
sd.beta = apply(beta[[1]], 2, sd)
ci.beta = posterior_interval(beta[[1]], prob = CI)
res.beta = cbind(mean.beta, median.beta, sd.beta, ci.beta)
nms = paste0("beta", 1:ncol(beta[[1]]))
res.beta
# results residual standard deviation
sigma = rstan::extract(object, pars = "sigma")
mean.sigma = mean(sigma[[1]])
median.sigma = median(sigma[[1]])
sd.sigma = sd(sigma[[1]])
ci.sigma = posterior_interval(as.matrix(sigma[[1]]), prob = CI)
res.sigma = c(mean.sigma, median.sigma, sd.sigma, ci.sigma)
res.sigma
# combine results
res = rbind(res.int, res.beta, res.sigma)
res
rownames(res) = c("int", nms, "sigma")
res
colnames(res) = c("mean", "median", "sd", names(ci.int)[1], names(ci.int)[2])
ci.int
names(ci.int)
rownames(ci.int)
head(ci.beta)
rownames(ci.beta)
nms.ci
1-CI
1-CI/2
(1-CI)/2
(1-CI)/2*100
paste0((1-CI)/2*100, "%")
(1-CI)/2*100
(CI+(1-CI)/2)*100
paste0((CI+(1-CI)/2)*100, "%")
nms.ci = c(paste0((1-CI)/2*100, "%"), paste0((CI+(1-CI)/2)*100, "%"))
colnames(res) = c("mean", "median", "sd", nms.ci)
res
CI = 0.9
nms.ci = c(paste0((1-CI)/2*100, "%"), paste0((CI+(1-CI)/2)*100, "%"))
nms.ci
0.5
CI <- 0.5
nms.ci = c(paste0((1-CI)/2*100, "%"), paste0((CI+(1-CI)/2)*100, "%"))
nms.ci
select_lm(fit, X = dat$x, prob = 0.9)
class(res)
res
##### Install package -----
setwd("~/Documents/bayesreg")
devtools::document() # update documentation
devtools::install(local=TRUE) # use FALSE if stan models need to be recompiled
library(bayesreg)
##### Simulate data -----
library(MASS) # for mvrnorm function in sim.lm
set.seed(16102018)
sim.lm <- function(nobs, cfs, sd, cov.mat){
nvar <- length(cfs)
beta <- as.matrix(cfs)
X <- mvrnorm(nobs, mu=rep(0, nvar), Sigma=cov.mat)
y <- X %*% beta + rnorm(nobs,0,sd)
return (list(x=X, y=y, coefficients=cfs, var=sd^2, covX=cov.mat))
}
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
colnames(dat$x) <- c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8")
# Create missingness
dat$x[c(1, 50), 1] <- NA
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
colnames(dat$x) <- c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8")
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "lasso", missing = FALSE, chains = 2)
select_lm(fit, X = dat$x, prob = 0.9)
pmse_lm(fit, y = dat$y, N_train = 40)
summary_lm(fit)
summary(fit, pars=c("mu", "beta", "sigma"))
summary_lm(fit)
summary(fit, pars=c("mu", "beta", "sigma"))$summary
?summary_lm
##### Install package -----
setwd("~/Documents/bayesreg")
devtools::document() # update documentation
devtools::install(local=TRUE) # use FALSE if stan models need to be recompiled
library(bayesreg)
##### Simulate data -----
library(MASS) # for mvrnorm function in sim.lm
set.seed(16102018)
sim.lm <- function(nobs, cfs, sd, cov.mat){
nvar <- length(cfs)
beta <- as.matrix(cfs)
X <- mvrnorm(nobs, mu=rep(0, nvar), Sigma=cov.mat)
y <- X %*% beta + rnorm(nobs,0,sd)
return (list(x=X, y=y, coefficients=cfs, var=sd^2, covX=cov.mat))
}
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
colnames(dat$x) <- c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8")
# Create missingness
dat$x[c(1, 50), 1] <- NA
dat$x[c(30, 42, 118, 201), 3] <- NA
dat$y[c(5, 8, 77, 110, 240)] <- NA
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "lasso", missing = FALSE, chains = 2)
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
colnames(dat$x) <- c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8")
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "lasso", missing = FALSE, chains = 2)
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "lasso", chains = 2)
select_lm(fit, X = dat$x, prob = 0.9)
pmse_lm(fit, y = dat$y, N_train = 40)
summary_lm(fit)
