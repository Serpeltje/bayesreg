p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghslam <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=2, scale=4)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghslam[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
p0 <- 53
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghsnu <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=2, scale=4)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=2, scale=4)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
require(tidyverse)
require(plyr)
require(distr) # for the spike-and-slab
require(LaplacesDemon) # for the regularized horseshoe
##### SAMPLE FROM PRIOR DENSITIES #####
N <- 1e+06
##### REGULARIZED HORSESHOE #####
Ng <- 2000
sh <- 2
sc <- 4
p0 <- 15
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghslam <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghslam[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
p0 <- 53
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghsnu <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
##### REGULARIZED HORSESHOE #####
Ng <- 1000
sh <- 2
sc <- 4
p0 <- 15
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghslam <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghslam[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
p0 <- 53
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghsnu <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
##### REGULARIZED HORSESHOE #####
Ng <- 3000
sh <- 2
sc <- 4
p0 <- 15
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghslam <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghslam[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
p0 <- 53
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghsnu <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
##### REGULARIZED HORSESHOE #####
Ng <- 52000
sh <- 2
sc <- 4
p0 <- 15
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghslam <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghslam[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
p0 <- 53
p <- 108
tau <- p0/((p-p0)*sqrt(Ng))
reghsnu <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghsnu[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
tau <- 1
reghs <- rep(NA, N)
for(i in 1:N){
c2 <- rinvgamma(1, shape=sh, scale=sc)
phi <- rhalfcauchy(1, scale=tau)
xi <- rhalfcauchy(1, scale=1)
xi.til2 <- c2*xi^2/(c2 + phi^2 * xi^2)
reghs[i] <- rnorm(1, mean=0, sd=sqrt(phi^2 * xi.til2))
}
plot(density(reghslam))
lines(density(reghs), col="red")
lines(density(reghsnu), col="blue")
?lapply
2623.11*250
2623.11*250/60/60
185/24
183/24
6*70
6*90
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetSeq", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
conditions
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
conditions
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
rm(list=ls())
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
set.seed(09082017)
require(glmnet) # for the ridge, lasso, and elastic net
require(grpreg) # for the group lasso
require(leaps) # to perform best subset selection
require(parallel)
## specify simulation conditions
methods <- c("ridge", "lasso", "elasticNet", "subsetFor")
cond <- 1:8
cond1 <- expand.grid(method = methods, condition = cond)
# group lasso is only applicable in conditions 3-6
methods <-  "groupLasso"
cond <- 3:6
cond2 <- expand.grid(method = methods, condition = cond)
conditions <- rbind.data.frame(cond1, cond2)
## simulation function
freq.pen <- function(pos, cond, reps){
mypath <- file.path("~/surfdrive/regularization_regression/additional_analyses")
### Select condition ###
condition <- cond$condition[pos]
method <- cond$method[pos]
### Load generated data ###
setwd(paste0(mypath, "/simdata"))
load(list.files()[grep(condition, list.files())])
simdata <- simdata.split[1:reps]
### Run ###
out <- lapply(simdata, function(x){
# output is similar for glmnet and grpreg
if(method %in% c("ridge", "lasso", "elasticNet", "groupLasso")){
if(method == "ridge"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0, intercept = TRUE, standardize = FALSE)
}
if(method == "lasso"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 1, intercept = TRUE, standardize = FALSE)
}
if(method == "elasticNet"){
fit <- cv.glmnet(x = x$trainX, y = x$trainY, alpha = 0.5, intercept = TRUE, standardize = FALSE)
}
if(method == "groupLasso"){
fit <- cv.grpreg(X = x$trainX, y = x$trainY, group = factor(c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 15))), penalty = "grLasso")
}
lambda.min <- fit$lambda.min # extract the lambda with the minimum CV error
cfs <- coef(fit) # extract coefficients at minimum CV error
# predict responses test set
if(method == "groupLasso"){
predY <- predict(fit, X = x$testX, which = lambda.min)
}
else({
predY <- predict(fit, newx=x$testX, s="lambda.min")
})
out <- list("name"=paste0(method, condition), "penalty_parameters" = lambda.min, "coefficients" = cfs, "predicted Y" = predY)
}
# run the subset selection and return coefficients based on several criteria, predict responses later (requires manual specification model)
if(method == "subsetFor"){
# create df with predictors & responses
datdf <- cbind.data.frame(x$trainY, x$trainX)
colnames(datdf) <- c("y", paste0("x", 1:(ncol(datdf)-1)))
fit <- regsubsets(y ~ ., datdf, intercept = TRUE, nbest = 1, nvmax = ncol(datdf)-1, method = "forward")
summ <- summary(fit)
cfs.adjr2 <- coef(fit, which.max(summ$adjr2))
cfs.cp <- coef(fit, which.min(summ$cp))
cfs.bic <- coef(fit, which.min(summ$bic))
out <- list("name"=paste0(method, condition), "coefficients adjusted R2" = cfs.adjr2, "coefficients Mallows' Cp" = cfs.cp, "coefficients BIC" = cfs.bic)
}
return(out)
})
}
nworkers <- detectCores() # number of cores to use
cl <- makePSOCKcluster(nworkers) # create cluster
clusterCall(cl, function() library(glmnet))
clusterCall(cl, function() library(grpreg))
clusterCall(cl, function() library(leaps))
system.time(out <- clusterApplyLB(cl, 1:nrow(conditions), freq.pen, cond=conditions, reps=2)) # run simulation
stopCluster(cl) # shut down the nodes
out
require(lavaan)
install.packages("lavaan")
require(lavaan)
load(HolzingerSwineford1939)
head(HolzingerSwineford1939)
unique(HolzingerSwineford1939[, 8])
length(unique(HolzingerSwineford1939[, 8]))
plot(HolzingerSwineford1939[, "x2"])
hist(HolzingerSwineford1939[, "x2"])
hist(HolzingerSwineford1939[, "x1"])
hist(HolzingerSwineford1939[, "x3"])
hist(HolzingerSwineford1939[, "x9"])
require(lavaan)
HS.model <- ' visual  =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed   =~ x7 + x8 + x9 '
fit <- cfa(HS.model, data=HolzingerSwineford1939)
lavInspect(fit, "sampstat")
covmat <- lavInspect(fit, "sampstat")
covmat$cov
solve(covmat$cov)
covmat$cov[,1]
cfs <- solve(covmat$cov) %*% covmat$cov[,1]
cfs
cfs <- solve(covmat$cov) %*% covmat$cov[,2]
solve(covmat$cov) %*% covmat$cov[,2]
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
summary(fit)
coef(fit)
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
0.809*0.554
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
0.549*0.809*0.554*1.134
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 2
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
# compute regression coefficients
j <- 1
solve(covmat$cov[-j, -j]) %*% covmat$cov[-j,j]
(.42 + .83 * 0)*(.04+.28*0)
(.42 + .83 * 1)*(.04+.28*1)
7.007/38.925
7.007/18
1.157/8
.883/8
.714^2 + .371^2
##### Install package -----
setwd("~/Documents/bayesreg")
devtools::install(local=TRUE) # use FALSE if stan models need to be recompiled
library(bayesreg)
##### Simulate data -----
library(MASS) # for mvrnorm function in sim.lm
set.seed(16102018)
sim.lm <- function(nobs, cfs, sd, cov.mat){
nvar <- length(cfs)
beta <- as.matrix(cfs)
X <- mvrnorm(nobs, mu=rep(0, nvar), Sigma=cov.mat)
y <- X %*% beta + rnorm(nobs,0,sd)
return (list(x=X, y=y, coefficients=cfs, var=sd^2, covX=cov.mat))
}
dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = matrix(0.5, ncol=8, nrow=8))
# Create missingness
dat$x[c(1, 50), 1] <- NA
dat$x[c(30, 42, 118, 201), 3] <- NA
dat$y[c(5, 8, 77, 110, 240)] <- NA
##### Test package -----
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = "ridge", missing = TRUE, chains = 2, iter = 2000)
posterior <- as.array(fit)
mcmc_dens(posterior, pars=c("beta[1]", "beta[2]")
)
bayesplot::mcmc_dens(posterior, pars=c("beta[1]", "beta[2]"))
?summary
require(rstan)
?summary
??summary
